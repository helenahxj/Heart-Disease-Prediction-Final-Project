---
title: "Heart Disease Prediction Final Project"
author: "Helena Hu"
date: "2022-12-11"
output: 
  html_document:
    toc: TRUE
    toc_depth: 2
    toc_float: TRUE
    fig_width: 8
    fig_height: 7
    code_folding: hide
---

<style type="text/css">
  body{
  font-size: 12pt;
}
</style>

![](CVD_NSHomepageWidget.jpeg)

# Introduction
The purpose of this project is to build a machine learning model that can predict whether people will developing heart disease. We will using [dataset](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset?resource=download) from Kaggle.

## Why is the project useful?
Predicting heart disease has significant meaning because it can help identify individuals who are at risk of developing the condition. This allows healthcare providers to take preventative measures, such as prescribing medication or making lifestyle changes, to reduce the likelihood of the individual developing heart disease and potentially save lives. Additionally, early detection of heart disease can lead to earlier treatment, which can improve the individual's chances of a successful treatment. Predicting heart disease can also help healthcare providers allocate resources more efficiently by focusing on individuals who are at the highest risk of developing the disease.

## Steps toward final model
Here are my plans on how to build machine learning model in this project:

1. Load necessary packages
2. Perform exploratory data analysis: cleaning and tidying the data and make bar plot between predictors and response variable
3. Building Models: Logistic Regression, LDA, QDA, Decision Tree, Random Forest, K-Nearest Neighbor
4. Compare the accuracy of each model and find the best one
5. Fitting the best model to testing data and making prediction

# Loading Packages
```{r}
library(corrplot) # for the correlation plot
library(rlang)
library(parsnip)
library(discrim)
library(corrr)  # for calculating correlation between variables
library(tidyverse)
library(tidymodels)
library(ggplot2) # for barplot
library(janitor) # for data cleaning
library(readr)
library(lubridate)
library(rpart.plot)
library(xgboost)

set.seed(1222)
```

# Exploratory Data Analysis
Now the dataset has been ready for next step analysis. First we will perform exploratory data analysis.

## Morphing Our Data
First, I read the CVS file of data, and I find that the name of variables in the dataset is not very clear since most of them are the abbreviations, so let's first change their name into understandable words. Then, I use `clean_names()` to convert all variable to snake case, which makes data set more tidy and neat with all the variable names consistent in style.

```{r}
heart <- read_csv("~/Desktop/heart disease final project/data/heart.csv")

names <- c("age",
           "sex",
           "chest pain type",
           "rest bp",
           "cholesterol",
           "fbs",
           "rest ECG",
           "max heart rate",
           "angina exercise",
           "ST depression",
           "ST slope",
           "num vessels",
           "thal",
           "heart disease")

colnames(heart) <- names

heart <- clean_names(heart) # data cleaning
```
The dataset include total of 14 variables, with 13 predictors and 1 response variables `heart_disease`. The variables and their descriptions are listed as follows:  

Variables | Descriptions | Value explanation
:-------- | :----------- | :----------------
age | Age in years | 
sex | Sex | 0: female, 1: male
chest_pain_type | Type of chest pain | 0: typical angina, 1: atypical angina, 2: non-anginal pain, 3: asymptomatic
rest_bp | Resting blood pressure in mm Hg on admission to the hospital | 
cholesterol | serum cholestoral in mg/dl | 
fbs | fasting blood sugar in mg/dl | 0: <= 120, 1: > 120
rest_ecg | Resting electrocardiographic results | 0: normal, 1: having ST-T wave abnormality, 2: showing probable or definite left ventricular hypertrophy by Estes' criteria
max_heart_rate | maximum heart rate achieved | 
angina_exercise | exercise induced angina | 0: no, 1: yes
st_depression | ST depression induced by exercise relative to rest | 
st_slope | The slope of the peak exercise ST segment |
num_vessels | number of major vessels (0-3) colored by flourosopy | 
thal | Form of Thalassemia | 1: normal, 2: fixed effect, 3:reversable effect
heart_disease | presence or not of heart disease (response variable) | 0: absence, 1: presence

## Tidying data
We will factorize the variable who has numerical value that represent different categories, and then we will look at the head of the data set to make sure the variables have changed to the correct type.
```{r}
heart$sex <- as.factor(heart$sex)
heart$chest_pain_type <- as.factor(heart$chest_pain_type)
heart$fbs <- as.factor(heart$fbs)
heart$rest_ecg <- as.factor(heart$rest_ecg)
heart$angina_exercise <- as.factor(heart$angina_exercise)
heart$thal <- as.factor(heart$thal)
heart$heart_disease <- as.factor(heart$heart_disease)

head(heart)
```

## Correlation Plot
First, I create a correlation plot to get an idea of their relationship. There's a strong positive correlation between 'st_depression' and `st_slope`, `max_heart_rate` and `age`. These make sense since `st_depression` and `st_slope` are related, and larger age means weaker vessels and heart function, which lead to higher maximum heart rate. There's a negative correlation between `age` and `num_vessels`. This is because older people are more likely to have less healthier vessels, so it end up with less vessels colored by flourosopy.

```{r}
heart_cor <- heart %>% 
  select_if(is.numeric) %>%
  cor() %>%
  corrplot()
```

Then, I create barplots for variables that can provide useful relationship between those variables and chance of developing heart disease, and according to graph we can conclude which are determined factors.

## Age
From the plot we can see people who are 40 years old to 55 years old are more likely to have heart disease. Normally, older people tend to have a less plastics blood vessels which makes them to become more susceptible to damage over time, so it is kind of surprising that younger people have higher risk of developing heart disease. 
```{r}
ggplot(heart, aes(age)) + 
  geom_bar(aes(fill = heart_disease)) +
  scale_fill_manual(values = c("black", "red"))
```

## Sex
The number of female is less than that of male in this dataset, but we still can see a pattern here. The percentage of female who have heart disease is much larger than that of male who have heart disease. There are more than half of male got heart disease.
```{r}
ggplot(heart, aes(sex)) + 
  geom_bar(aes(fill = heart_disease), position=position_dodge()) +
  scale_fill_manual(values = c("black", "red"))
```

## Chest pain type
We can see from the plot that people with atypical angina, non-anginal, and asymptomatic chest pain has greatest risk of developing heart disease. Though there are people who have heart disease has typical angina chest pain, the effect of it is much less significant compare to other types of chest pain.
```{r}
ggplot(heart, aes(chest_pain_type)) + 
  geom_bar(aes(fill = heart_disease), position=position_dodge()) +
  scale_fill_manual(values = c("black", "red"))
```

## Resting Blooding Pressure
There isn't much relation between resting blooding pressure and chance of developing heart disease. The portion of people having heart disease at each level of resting blooding pressure are similar, except for few outliers. 
```{r}
ggplot(heart, aes(rest_bp)) + 
  geom_bar(aes(fill = heart_disease)) +
  scale_fill_manual(values = c("black", "red"))
```


## Serum Cholestoral
People having heart disease mostly have serum cholestoral between 200 mg/dl to 300 mg/dl, with few expceptions at about 400 mg/dl and more than 550 mg/dl.
```{r}
ggplot(heart, aes(cholesterol)) + 
  geom_bar(aes(fill = heart_disease)) +
  scale_fill_manual(values = c("black", "red"))
```


## Fasting Blood Sugar
There isn't much relation between fasting blood sugar and presence of heart disease. The percentage of people who have heart disease for whether the fasting blood suger is larger than 120 or not are both near 50%, so we can conclude that fasting blood sugar is not a determined factor for the presence of heart disease.
```{r}
ggplot(heart, aes(fbs)) + 
  geom_bar(aes(fill = heart_disease), position=position_dodge()) +
  scale_fill_manual(values = c("black", "red"))
```


## Resting Electrocardiographic Results
We can see from the plot, compare to other resting ECG type, people having ST-T wave abnormality are more like to have heart disease present. It has more than a half of them developing heart disease.
```{r}
ggplot(heart, aes(rest_ecg)) + 
  geom_bar(aes(fill = heart_disease), position=position_dodge()) +
  scale_fill_manual(values = c("black", "red"))
```

## Maximum Heart Rate
From the plot, we can see a strong pattern. People with higher maximum heart rate tend to have a much higher possibility to develop heart disease. We can conclude that maximum heart rate is a major factor for the presence of heart disease.
```{r}
ggplot(heart, aes(max_heart_rate)) + 
  geom_bar(aes(fill = heart_disease)) +
  scale_fill_manual(values = c("black", "red"))
```

## Exercise Induced Angina
Exercise with angina can help reduce the risk of developing heart disease, as there's greater portion of people who exercise without inducing angina have heart disease.
```{r}
ggplot(heart, aes(angina_exercise)) + 
  geom_bar(aes(fill = heart_disease), position=position_dodge()) +
  scale_fill_manual(values = c("black", "red"))
```

## Number of Major Vessels Colored by Flourosopy
People with least major vessels colored by flourosopy have significant chance gettig heart disease. It might be explained by that only healthier vessels can be seen by flourosopy, so people with 0 vessels seen by flourosopy means that they have weaker vessels than others, which lead to a higher risk of developing heart disease.
```{r}
ggplot(heart, aes(num_vessels)) + 
  geom_bar(aes(fill = heart_disease), position=position_dodge()) +
  scale_fill_manual(values = c("black", "red"))
```

## Form of Thalassemia
Fixed effect form of thalassemia have most significant influence on heart disease and reversable effect form of thalassemia have the least influence on heart disease. This make sense since fixed effect of thalassemia is more severe than reversable as explained by the name of the forms that fixed effect cannot be altered.
```{r}
ggplot(heart, aes(thal)) + 
  geom_bar(aes(fill = heart_disease), position=position_dodge()) +
  scale_fill_manual(values = c("black", "red"))
```

# Model Building
We will start building our model. I choose 5 models: Logistic Regression, LDA, QDA, Decision Tree, Random Forest, and K-Nearest Neighbor.

## Train/Test Split
Before actually setting up the models, first we'll have to split our data into training split and testing split. Here I choose `prop = 0.8` and stratify on response variable `heart_disease`. I use stratified sampling to make sure there will be a more precise estimate later when fitting the models.

```{r}
set.seed(1222)

heart_split <- heart %>%
  initial_split(prop = 0.8, strata = "heart_disease")

heart_train <- training(heart_split) # training split
heart_test <- testing(heart_split) # testing split
```

```{r}
dim(heart_train)
```

```{r}
dim(heart_test)
```

## Recipe Building
We will use the same predictors and response variables for all models. For the recipe, we will use all predictors since they are all relevant and has influence on our response variables. Using all predictors can make sure we can build models with highest accuracy.
```{r}
heart_recipe <- recipe(heart_disease ~ ., data = heart_train) %>% 
  step_dummy(all_nominal_predictors()) %>% # dummy predictor on nominal variables
  step_center(all_predictors()) %>%   # standardizing predictors
  step_scale(all_predictors())
```

## K-Fold Cross Validation
Here I use stratified cross validation by stratify on response variable `heart_disease` to deal with imbalanced data. 
```{r}
heart_folds <- vfold_cv(heart_train, v = 10, strata = heart_disease) # 10-folds CV
```

```{r}
save(heart_folds, heart_recipe, heart_train, file = "~/Desktop/heart disease final project/model/model_set_up.rda")
```

## Models
Now we are ready to build five models. For each model, I follow the same steps: first I will set up the model and set its engine and mode. The mode will always be "classification" since there's only two possible categorical outcome for response variable, `heart_disease`. Second, I will set up workflow and add the model and recipe established above. Then, I will fit each model with workflow to the training data I split at the beginning. 

For logistics regression, LDA, and QDA, I will use `predict()` to access the performance of each model.
For decision tree, random forest, and k-nearest neighbor, I will further set up a tune grid with tuned parameter and level, and then tune the model and select the most accurate model. 

For each model, I saved result and workflow.

## Logistic Regression Model
```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_workflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(heart_recipe)

log_fit <- fit(log_workflow, heart_train)

predict(log_fit, new_data = heart_train, type = "prob")

save(log_fit, log_workflow, file = "~/Desktop/heart disease final project/model/log_regression.rda")
```

## LDA Model
```{r}
lda_model <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_workflow <- workflow() %>% 
  add_model(lda_model) %>% 
  add_recipe(heart_recipe)

lda_fit <- fit(lda_workflow, heart_train)

predict(lda_fit, new_data = heart_train, type = "prob")

save(lda_fit, lda_workflow, file = "~/Desktop/heart disease final project/model/LDA.rda")
```

## QDA Model
```{r}
qda_model <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_workflow <- workflow() %>% 
  add_model(qda_model) %>% 
  add_recipe(heart_recipe)

qda_fit <- fit(qda_workflow, heart_train)

predict(qda_fit, new_data = heart_train, type = "prob")

save(qda_fit, qda_workflow, file = "~/Desktop/heart disease final project/model/QDA.rda")
```

## Decision Tree Model
```{r}
dt_model <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

dt_workflow <- workflow() %>% 
  add_model(dt_model) %>% 
  add_recipe(heart_recipe)

dt_fit <- dt_model %>%
  fit(heart_disease ~ ., data = heart_train)

dt_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 2)
dt_tune <- dt_workflow %>%
  tune_grid(resamples = heart_folds, grid = dt_grid) # tuning the model

save(dt_tune, dt_workflow, file = "~/Desktop/heart disease final project/model/Decision_Tree.rda")
```

```{r}
show_best(dt_tune, metric = "roc_auc") %>% select(-.estimator, -.config)
```
The best estimate of decision tree model would be at mean equal to 0.8857097.

## Random Forest Model
```{r}
rf_model <- rand_forest(min_n = tune(), mtry = tune(), mode = "classification") %>%
  set_engine("ranger")

rf_workflow <- workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(heart_recipe)

rf_fit <- fit(rf_model, heart_disease ~ ., data = heart_train)

rf_params <- parameters(rf_model) %>% # choose paramters
  update(mtry = mtry(range= c(2, 120)))

rf_grid <- grid_regular(rf_params, levels = 2)

rf_tune <- rf_workflow %>% 
  tune_grid(resamples = heart_folds, grid = rf_grid)

save(rf_tune, rf_workflow, file = "~/Desktop/heart disease final project/model/Random_Forest.rda")
```

```{r}
autoplot(rf_tune)
```

We can see from the plot that roc_auc increase slightly as the number of randomly selected predictors increases.

```{r}
show_best(rf_tune, metric = "roc_auc") %>% select(-.estimator, -.config)
```
The smallest mean is 0.9688 with `mtry = 2` and `min_n = 40`, and the largest mean is 0.999, which is really close to 1, with `mtry = 120` and `min_n = 2`.

## K-Nearest Neighbor Model
```{r}
knn_model <- nearest_neighbor(neighbors = tune(), mode = "classification") %>%
  set_engine("kknn")

knn_workflow <- workflow() %>% 
  add_model(knn_model) %>% 
  add_recipe(heart_recipe)

knn_fit <- fit(knn_workflow, heart_train)

knn_params <- parameters(knn_model) # choose parameters

knn_grid <- grid_regular(knn_params, levels = 2)

knn_tune <- knn_workflow %>%
  tune_grid(resamples = heart_folds, grid = knn_grid) # tuning the model

save(knn_tune, knn_workflow, file = "~/Desktop/heart disease final project/model/KNN.rda")
```

```{r}
autoplot(knn_tune)
```

The plot show the roc_auc decrese as number of neighbors increase, and it can be confirmed with `show_best()` function shown below, with mean being smallest when `neighbors = 15`.

```{r}
show_best(knn_tune, metric = "roc_auc") %>% select(-.estimator, -.config)
```

## Accuracy of Models
I created a tibble that shows ROC AUC score for each model in decending order to compare the accuracy of them. From the result we can find out which is the best model.
```{r}
log_auc <- augment(log_fit, new_data = heart_train) %>%
  roc_auc(heart_disease, estimate = .pred_0) %>%
  select(.estimate)

lda_auc <- augment(lda_fit, new_data = heart_train) %>%
  roc_auc(heart_disease, estimate = .pred_0) %>%
  select(.estimate)

qda_auc <- augment(qda_fit, new_data = heart_train) %>%
  roc_auc(heart_disease, estimate = .pred_0) %>%
  select(.estimate)

dt_auc <- augment(dt_fit, new_data = heart_train) %>%
  roc_auc(heart_disease, estimate = .pred_0) %>%
  select(.estimate)

rf_auc <- augment(rf_fit, new_data = heart_train) %>%
  roc_auc(heart_disease, estimate = .pred_0) %>%
  select(.estimate)

knn_auc <- augment(knn_fit, new_data = heart_train) %>%
  roc_auc(heart_disease, estimate = .pred_0) %>%
  select(.estimate)

heart_roc_aucs <- c(log_auc$.estimate,
                    lda_auc$.estimate,
                    qda_auc$.estimate,
                    dt_auc$.estimate,
                    rf_auc$.estimate,
                    knn_auc$.estimate)

heart_mod_names <- c("Logistic Regression",
                     "LDA",
                     "QDA",
                     "Decision Tree",
                     "Random Forest",
                     "K-Nearest Neighbor")
```

```{r}
heart_results <- tibble(Model = heart_mod_names,
                             ROC_AUC = heart_roc_aucs)

heart_results <- heart_results %>% 
  arrange(-heart_roc_aucs)

heart_results
```

# Best Model
Decision Tree Model has the highest ROC AUC score, which makes it to be the best performer. We will then work on decision tree model to make predictions. First, here is the visualization of the decision tree using function in `rpart.plot` package.
```{r}
dt_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

Let's find the best estimate of the decision tree model using `show_best()`, as we have already did after tuning the model.
```{r}
show_best(dt_tune, metric = "roc_auc") %>% # show the best decision tree model
  select(-.estimator, .config)
```

# Finalize Model
Now it's time to finalize the workflow from decision tree model, and then fit the finalized workflow to our training set. Still, I will save the final result.
```{r}
dt_workflow_tuned <- dt_workflow %>% 
  finalize_workflow(select_best(dt_tune, metric = "roc_auc"))

dt_results <- fit(dt_workflow_tuned, heart_train)

save(dt_results, file = "~/Desktop/heart disease final project/model/Final.rda")
```

## Final ROC AUC Results
```{r}
heart_roc_auc <- augment(dt_fit, new_data = heart_test) %>%
  roc_auc(heart_disease, estimate = .pred_0) %>%
  select(.estimate)

heart_roc_auc
```
The final roc auc score is 0.914717. We can conclude that our model performs pretty well based on the fact that the roc auc score is larger than 0.9. Now it's time to make some predictions!

## Fitting model to testing data and making prediction
We have find the best fit and now we will fit it to our testing data. After the fitting, we can make prediction based on that, and we add the prediction to the actual value.
```{r}
heart_predict <- predict(dt_fit,
                         new_data = heart_test, 
                         type = "class") # fitting model to testing data

heart_actual_predict <- heart_predict %>%
  bind_cols(heart_test)

heart_actual_predict
```

## Check Predictions
We will extract some values from the original data to examine the performance of our model. I take the values from the first line and the last line of the data.

### First Prediction
Our model predict the first patient will not have heart disease, and it is correct according to the original data.
```{r}
heart_example1 <- data.frame(
  age = 53,
  sex = "1",
  chest_pain_type = "0",
  rest_bp = 140,
  cholesterol = 203,
  fbs = "1",
  rest_ecg = "0",
  max_heart_rate = 155,
  angina_exercise = "1",
  st_depression = 3.1,
  st_slope = 0,
  num_vessels = 0,
  thal = "3"
)
```

```{r}
predict(dt_fit, heart_example1, type = "class")
```

### Last Prediction
Our model did a great prediction! It successfully predict the presence of the heart disease for this patient, as confirmed by our orginial data.
```{r}
heart_example2 <- data.frame(
  age = 59,
  sex = "1",
  chest_pain_type = "1",
  rest_bp = 140,
  cholesterol = 221,
  fbs = "0",
  rest_ecg = "1",
  max_heart_rate = 164,
  angina_exercise = "1",
  st_depression = 0.0,
  st_slope = 2,
  num_vessels = 0,
  thal = "2"
)
```

```{r}
predict(dt_fit, heart_example2, type = "class")
```
# Conclusion
After performing model building and relevant analysis, we find the decision tree model to be the best model for prediction of the presence of heart disease given patients health informations. 

Our model performs pretty well, but there are still make improvements on some aspect. There might be some inaccuracy due to the insufficient samples. For example, the predictor `sex` has mostly being male. The amount of male being studied is almost twice of amount of female being studied. Also, there's few data about patient who have asymptomatic chest pain compare to other chest pain patient. We probability cannot find useful relationship between those variables with specific value and the presence of heart disease as there isn't enough diversity. We might be able to find a perfect model if we can gather more information.

In conclusion, the model we build in this project learn to identify patterns and make predictions about a patient's likelihood of having heart disease, and we can always improve the model by using a larger and more diverse dataset. This project gave me an excellent experience by putting me in a practical biostatistics-related scenario, which is my field of interest, and allow me to apply and improve my statistical skills in real life problems. More importantly, this project has significant practical meaning: the final model would be of great help for healthcare providers to make precautionary measures to save the lives of those patient who have potential risk of having heart disease.
